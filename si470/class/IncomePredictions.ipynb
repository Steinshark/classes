{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6187eb",
   "metadata": {},
   "source": [
    "## Classification Project \n",
    "\n",
    "### Everett Stenberg\n",
    "### Antawn Weg\n",
    "In this project, my group explores the uses of using logistic regressors and SVMs to classify the income of an individual given their census data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f603ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial imports out \n",
    "import pandas \n",
    "import numpy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3c376",
   "metadata": {},
   "source": [
    "Here we will make some initial initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f324bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"data.csv\",header=None)\n",
    "feature_set = [] #find unique features to be one-hot encoded \n",
    "new_dataset = [] #beginnings of our final prepared data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6deff1b",
   "metadata": {},
   "source": [
    "At this point, we wanted to reduce the number of one-hot encodings of the place of origin, since it may lead to overfitting or bad predictions. Here, we assigned general regions to the countries in order to reduce the feature size we will eventually have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3c9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "US = [' United-States']\n",
    "\n",
    "NA = [' Puerto-Rico',' Haiti',' Mexico',' Cuba',' Canada',' Amer-Indian-Eskimo',\n",
    "      ' Haiti',' Dominican-Republic',' Jamaica',' Outlying-US(Guam-USVI-etc)']\n",
    "\n",
    "ASIA = [' Iran',' Philippines',' Cambodia',' Thailand',' Laos',\n",
    "        ' Taiwan',' China',' India',' Japan',' Vietnam',' Hong']\n",
    "\n",
    "EU = [' England',' Germany',' Italy',' Poland',' France',' Portugal',' Yugoslavia',' Greece',' Hungary',\n",
    "      ' Holand-Netherlands',' Scotland',' Ireland']\n",
    "\n",
    "SA = [' Columbia',' Ecuador',' El-Salvador',' Honduras',' Guatemala',' Peru',' Nicaragua',' Trinadad&Tobago']\n",
    "\n",
    "OTHER = [ ' Other',' South', ' ?']\n",
    "\n",
    "regions = {'US':US,'NA':NA,'ASIA':ASIA,'EU':EU,'SA':SA,'OTHER':OTHER}\n",
    "\n",
    "feature_set += list(regions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc2cdc",
   "metadata": {},
   "source": [
    "Similarly, we wanted to make education a number, which further reduced the size of features and provided more meaningful data than simply one-hot encoding a category of education. Approximate years of education has a cumulative, scalar affect, so I felt this better suited the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee50fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "education = {\n",
    "    ' Bachelors': 17,\n",
    "    ' HS-grad': 13,\n",
    "    ' 11th': 12,\n",
    "    ' Masters': 19,\n",
    "    ' 9th': 10,\n",
    "    ' Some-college': 15,\n",
    "    ' Assoc-acdm': 15,\n",
    "    ' Assoc-voc': 15,\n",
    "    ' 7th-8th': 8.5,\n",
    "    ' Doctorate': 23,\n",
    "    ' Prof-school': 19,\n",
    "    ' 5th-6th': 6.5,\n",
    "    ' 10th': 11,\n",
    "    ' 1st-4th': 3.5,\n",
    "    ' Preschool': 0,\n",
    "    ' 12th': 13}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce51e29",
   "metadata": {},
   "source": [
    "## Find all one-hot encoding features \n",
    "\n",
    "Here, we'll go through our data and check all values of features that will need to be one hot encoded. If we find a unique value (i.e. a new member of the discrete set of relationship types), then well add it to the list of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91be47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataframe.iterrows():\n",
    "    datapoint = list(row[1])\n",
    "    if not datapoint[1] in feature_set:\n",
    "        feature_set.append(datapoint[1])\n",
    "    elif not datapoint[5] in feature_set:\n",
    "        feature_set.append(datapoint[5])\n",
    "    elif not datapoint[6] in feature_set:\n",
    "        feature_set.append(datapoint[6])\n",
    "    elif not datapoint[7] in feature_set:\n",
    "        feature_set.append(datapoint[7])\n",
    "    elif not datapoint[8] in feature_set:\n",
    "        feature_set.append(datapoint[8])\n",
    "        \n",
    "    for r in regions:\n",
    "        if datapoint[13] in regions[r]:\n",
    "            datapoint[13] = r\n",
    "        \n",
    "    # Add to preparing data\n",
    "    new_dataset.append(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bca1ae",
   "metadata": {},
   "source": [
    "## Clean and Format the Dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cffaf",
   "metadata": {},
   "source": [
    "Here, we either properly cast the feature for storage in the dataset from the original, or we do a one-hot encoded feature lookup and apply the proper encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4851a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dp in enumerate(new_dataset):\n",
    "    \n",
    "    # initialize a new dict  \n",
    "    formatted_dp = {}\n",
    "    # Add manual features \n",
    "    formatted_dp[\"age\"] = int(dp[0])\n",
    "    formatted_dp[\"fnlwgt\"] = int(dp[2])\n",
    "    formatted_dp[\"ed_num\"] = int(dp[4])\n",
    "    formatted_dp[\"gender\"] = int((dp[9] == \"Male\"))\n",
    "    formatted_dp[\"capital_gain\"] = int(dp[10])\n",
    "    formatted_dp[\"capital_loss\"] = int(dp[11])\n",
    "    formatted_dp[\"hours_per_week\"] = int(dp[12])\n",
    "    formatted_dp[\"over50k\"] = int(dp[-1].strip()[0] == '>')\n",
    "    # Translate education level\n",
    "    formatted_dp[\"education_years\"] = education[dp[3]] \n",
    "    \n",
    "    # Add one-hot encoding features \n",
    "    for feat in feature_set:\n",
    "        if feat in dp:\n",
    "            formatted_dp[feat] = 1\n",
    "        else:\n",
    "            formatted_dp[feat] = 0   \n",
    "    new_dataset[i] = formatted_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31b28f",
   "metadata": {},
   "source": [
    "## Data encoding:\n",
    "\n",
    "### Continuous Values \n",
    "    fnlwgt : The final number the census decided on (no idea??)\n",
    "    ed_num : No idea what this represents \n",
    "    gender : 0 == male, 1 == female \n",
    "    \n",
    "### One Hot Encoded  \n",
    "    Occupation, Race, Region of Origin , Maritial Status, Relationship Status, Familty Status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ff1c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 52,\n",
       " 'fnlwgt': 209642,\n",
       " 'ed_num': 9,\n",
       " 'gender': 0,\n",
       " 'capital_gain': 0,\n",
       " 'capital_loss': 0,\n",
       " 'hours_per_week': 45,\n",
       " 'over50k': 1,\n",
       " 'education_years': 13,\n",
       " 'US': 1,\n",
       " 'NA': 0,\n",
       " 'ASIA': 0,\n",
       " 'EU': 0,\n",
       " 'SA': 0,\n",
       " 'OTHER': 0,\n",
       " ' State-gov': 0,\n",
       " ' Self-emp-not-inc': 1,\n",
       " ' Private': 0,\n",
       " ' Married-civ-spouse': 1,\n",
       " ' Prof-specialty': 0,\n",
       " ' Exec-managerial': 1,\n",
       " ' Married-spouse-absent': 0,\n",
       " ' Husband': 1,\n",
       " ' Never-married': 0,\n",
       " ' White': 1,\n",
       " ' Black': 0,\n",
       " ' Asian-Pac-Islander': 0,\n",
       " ' Adm-clerical': 0,\n",
       " ' Sales': 0,\n",
       " ' Craft-repair': 0,\n",
       " ' Transport-moving': 0,\n",
       " ' Farming-fishing': 0,\n",
       " ' Machine-op-inspct': 0,\n",
       " ' Divorced': 0,\n",
       " ' Separated': 0,\n",
       " ' Federal-gov': 0,\n",
       " ' Tech-support': 0,\n",
       " ' Local-gov': 0,\n",
       " ' Own-child': 0,\n",
       " ' ?': 0,\n",
       " ' Not-in-family': 0,\n",
       " ' Protective-serv': 0,\n",
       " ' Other-service': 0,\n",
       " ' Unmarried': 0,\n",
       " ' Married-AF-spouse': 0,\n",
       " ' Handlers-cleaners': 0,\n",
       " ' Wife': 0,\n",
       " ' Self-emp-inc': 0,\n",
       " ' Other-relative': 0,\n",
       " ' Widowed': 0,\n",
       " ' Amer-Indian-Eskimo': 0,\n",
       " ' Other': 0,\n",
       " ' Armed-Forces': 0,\n",
       " ' Priv-house-serv': 0,\n",
       " ' Without-pay': 0,\n",
       " ' Never-worked': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A look at our dataset\n",
    "new_dataset[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08621da7",
   "metadata": {},
   "source": [
    "Now, well compile our data, clean and format it, and turn it into train and test sets - which are numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b46b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### reorder the data correctly since it was in a dict \n",
    "ordered_data = []\n",
    "lengths = {}\n",
    "\n",
    "for d in new_dataset:  \n",
    "    \n",
    "    # Create the datapoint\n",
    "    classification = d['over50k']\n",
    "    d_as_l = [d[f] for f in d if not f == 'over50k']\n",
    "    final_datapoint = [classification] + d_as_l\n",
    "    \n",
    "    # Add to our datalist\n",
    "    ordered_data.append(final_datapoint)\n",
    "\n",
    "    \n",
    "# Create a numpy array\n",
    "data_arr = numpy.array([numpy.array(l) for l in ordered_data])\n",
    "\n",
    "# Shuffle the data\n",
    "numpy.random.shuffle(data_arr)\n",
    "x = int(.7 * len(data_arr))\n",
    "# Pick 70% for training \n",
    "x_train, y_train, x_test, y_test = data_arr[:x,1:], data_arr[:x,0], data_arr[x:,1:], data_arr[x:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a717c8b",
   "metadata": {},
   "source": [
    "### Lets make a logistics model to try to predict with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3c4a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7930187327259699\n",
      "Confusion matrix:\n",
      "[[7140  247]\n",
      " [1775  607]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Classifier for the dataset \n",
    "l_model = LogisticRegression(max_iter=1000)\n",
    "l_model.fit(x_train,y_train)\n",
    "# Get the output\n",
    "lreg_out = l_model.predict(x_test)\n",
    "print(f\"accuracy: {metrics.accuracy_score(lreg_out,y_test)}\")\n",
    "print(f\"Confusion matrix:\\n{metrics.confusion_matrix(y_test,lreg_out)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4679c6",
   "metadata": {},
   "source": [
    "I do not consider this to be a bad score! We can accurately predict 4 out of 5 times if someones median income is above or below 50k per year given only limited data features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8898c",
   "metadata": {},
   "source": [
    "### Now, lets get an SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1280b702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lin model;\tModel fit in 71.12s , accuracy:  lin: 0.852\n",
      "Confusion matrix:\n",
      "[[6937  450]\n",
      " [ 999 1383]]\n",
      "\n",
      "running rbf model;\tModel fit in 43.98s , accuracy:  rbf: 0.852\n",
      "Confusion matrix:\n",
      "[[6960  427]\n",
      " [1023 1359]]\n",
      "\n",
      "running poly model;\tModel fit in 40.98s , accuracy:  poly: 0.839\n",
      "Confusion matrix:\n",
      "[[6920  467]\n",
      " [1105 1277]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll try several different kernels\n",
    "svm_models = {'lin':svm.SVC(kernel='linear',gamma='auto'),'rbf':svm.SVC(kernel='rbf',gamma='auto'),'poly':svm.SVC(kernel='poly',gamma='auto')}\n",
    "\n",
    "for model in svm_models:\n",
    "    t1 = time()\n",
    "    print(f'running {model} model',end=';\\t')\n",
    "    pipe = make_pipeline(StandardScaler(),svm_models[model])\n",
    "    pipe.fit(x_train,y_train)\n",
    "    print(f\"Model fit in {(time()-t1):.2f}s , accuracy: \",end=' ')\n",
    "    svm_out = pipe.predict(x_test)\n",
    "    print(f'{model}: {metrics.accuracy_score(svm_out,y_test):.3f}')\n",
    "    print(f\"Confusion matrix:\\n{metrics.confusion_matrix(y_test,svm_out)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f67b1a",
   "metadata": {},
   "source": [
    "Our results seem pretty good! An ~85% percent accuracyrate is not too bad for the SVM model given our limited data! It clearly does a much better job of predicting the data versus the log regressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
